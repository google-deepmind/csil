{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKvuSeZ0vGqk"
      },
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pgXwbwzSKRl"
      },
      "source": [
        "### **Discrete tabular experiments for evaluating imitation learning algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06cUc_uRL903"
      },
      "outputs": [],
      "source": [
        "# @title Setup and define MDPs\n",
        "\n",
        "SPARSE_INITIAL_STATES = False\n",
        "EASY_REWARD = True\n",
        "EPS = 1e-6  # avoid nans\n",
        "\n",
        "import jax\n",
        "from jax import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def softmax(values, temperature, prior_policy):\n",
        "  log_values = values / temperature + jnp.log(prior_policy)\n",
        "  # return jnp.exp(log_values - logsumexp(log_values, axis=1, keepdims=True))\n",
        "  return jax.nn.softmax(log_values, axis=1)\n",
        "\n",
        "def soft_policy_iteration(reward, transition, initial,  prior_policy, alpha, gamma, n_iters=1000, pi_init=None, q_init=None):\n",
        "  d_state, d_action, _ = transition.shape\n",
        "  q = jnp.zeros((d_state, d_action)) if q_init is None else q_init\n",
        "  pi = jnp.ones(((d_state, d_action))) / d_action if pi_init is None else pi_init\n",
        "\n",
        "  def step(pi, q):\n",
        "    pi_kl = jnp.sum(pi * (jnp.log(prior_policy) - jnp.log(EPS + pi)), axis=1)\n",
        "    v = jnp.sum(q * pi, axis=1) + alpha * pi_kl\n",
        "    q = reward + gamma * jnp.sum(transition * jnp.tile(v, (d_state, d_action, 1)), 2) + (1-gamma) * jnp.sum(initial * v)\n",
        "    pi = softmax(q, alpha, prior_policy)\n",
        "    return pi, q, v\n",
        "\n",
        "  step = jax.jit(step)\n",
        "\n",
        "  for _ in tqdm(range(n_iters)):\n",
        "    pi, q, v = step(pi, q)\n",
        "  return pi, q, v\n",
        "\n",
        "def stationary_distribution(initial, transition, policy, discount, n_iters=1000):\n",
        "  distribution = initial\n",
        "\n",
        "  def sim(distribution):\n",
        "    distribution_ = discount * jnp.einsum(\"ijk,ij,i-\u003ek\", transition, policy, distribution) + (1 - discount) * initial\n",
        "    distribution_ = distribution_ / distribution_.sum()\n",
        "    delta = jnp.abs(distribution_ - distribution).max()\n",
        "    return distribution_, delta\n",
        "\n",
        "  sim = jax.jit(sim)\n",
        "  for _ in range(n_iters):\n",
        "    distribution, delta = sim(distribution)\n",
        "    if delta \u003c 1e-4:\n",
        "      break\n",
        "  else:\n",
        "    print(\"Didn't converge!\")\n",
        "  return distribution\n",
        "\n",
        "def simulate(initial, transition, policy, absorbing_states, n_episodes):\n",
        "  t_max = 5_000\n",
        "  key = jax.random.PRNGKey(0)\n",
        "  d_state, d_action = policy.shape\n",
        "  experience = np.zeros((t_max, d_state, d_action))\n",
        "  state = jax.random.choice(key, jnp.arange(d_state), p=initial)\n",
        "  n, t = 0, 0\n",
        "  while n \u003c n_episodes:\n",
        "    _, key = jax.random.split(key)\n",
        "    # act greedy\n",
        "    action = jnp.argmax(policy[state])\n",
        "    experience[t, state, action] = experience[t, state, action] + 1\n",
        "    next_state_dist = transition[state, action, :]\n",
        "    next_state = jax.random.choice(key, jnp.arange(d_state), p=next_state_dist)\n",
        "    t += 1\n",
        "    if state in absorbing_states:\n",
        "      state = jax.random.choice(key, jnp.arange(d_state), p=initial)\n",
        "      n += 1\n",
        "    else:\n",
        "      state = next_state\n",
        "  return experience[:t, :, :]\n",
        "\n",
        "d_action = 4\n",
        "width = 10\n",
        "discount = 0.95\n",
        "gamma = discount\n",
        "d_state = width * width\n",
        "reward = jnp.zeros((d_state, d_action))\n",
        "if EASY_REWARD:\n",
        "  absorbing_state = [13 * width // 4,\n",
        "                     15 * width // 4,\n",
        "                     29 * width // 4,\n",
        "                     31 * width // 4]\n",
        "  for s in absorbing_state:\n",
        "    reward = reward.at[s, :].set(1.)\n",
        "\n",
        "else:  # sparse positive and negative\n",
        "  reward = reward.at[-width, :].set(1. / (1 - discount))\n",
        "  reward = reward.at[3 * width, :].set(-1. / (1 - discount))\n",
        "  absorbing_state = [d_state - width,]\n",
        "\n",
        "policy_eval = lambda eval_policy, state_dist: (reward * (jnp.diag(state_dist) @ eval_policy)).sum()\n",
        "\n",
        "def make_dynamics(width, d_state, d_action, initial_idxs, windy):\n",
        "  transition_matrix = jnp.zeros((d_state, d_action, d_state))\n",
        "  clamp = lambda coord: max(min(coord, width - 1), 0)\n",
        "  idx_to_coord = lambda idx: (i % width, i // width)\n",
        "  coord_to_idx = lambda x, y:  clamp(y) * width + clamp(x)\n",
        "  for i in range(d_state):\n",
        "    x, y = idx_to_coord(i)\n",
        "    next_states = jnp.array(list(map(coord_to_idx, [x+1, x-1, x, x], [y, y, y+1, y-1])))\n",
        "    for j, k in zip(jnp.arange(d_action), next_states):\n",
        "      if i not in absorbing_state:\n",
        "        transition_matrix = transition_matrix.at[i, j, k].set(1.)\n",
        "      else: # state is absorbing\n",
        "        transition_matrix = transition_matrix.at[i, j, initial_idxs].set(1.)\n",
        "    if windy and i != (d_state - width):\n",
        "      disturance_idx = coord_to_idx(x+1, y)\n",
        "      for j in range(d_action):\n",
        "        transition_matrix = transition_matrix.at[i, j, disturance_idx].set(1.0)\n",
        "  transition_matrix = transition_matrix / transition_matrix.sum(axis=2, keepdims=True)\n",
        "  return transition_matrix\n",
        "\n",
        "if SPARSE_INITIAL_STATES:\n",
        "  initial_idxs = jnp.zeros((1,)).astype(jnp.int32)\n",
        "  initial = jnp.zeros((d_state))\n",
        "  initial = initial.at[0].set(1.)\n",
        "else:\n",
        "  initial_idxs = jnp.arange(d_state)\n",
        "  initial = jnp.ones((d_state))\n",
        "\n",
        "initial = initial / initial.sum()\n",
        "\n",
        "transition_matrix = make_dynamics(width, d_state, d_action, initial_idxs, windy=False)\n",
        "\n",
        "windy_transition_matrix = make_dynamics(width, d_state, d_action, initial_idxs, windy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wh0ff3j5OSAL"
      },
      "outputs": [],
      "source": [
        "# @title Expert (SPI)\n",
        "\n",
        "expert_alpha = 0.1\n",
        "prior_policy = jnp.ones((d_state, d_action)) / d_action\n",
        "expert_pi, expert_q, expert_v = soft_policy_iteration(reward, transition_matrix, initial, prior_policy, alpha=expert_alpha, gamma=discount)\n",
        "expert_rho = stationary_distribution(initial, transition_matrix, expert_pi, discount)\n",
        "expert_windy_rho = stationary_distribution(initial, windy_transition_matrix, expert_pi, discount)\n",
        "\n",
        "experience = simulate(initial, transition_matrix, expert_pi, absorbing_state, n_episodes=100)\n",
        "expert_counts = experience.sum(0)\n",
        "expert_distribution = expert_counts / expert_counts.sum(keepdims=True)\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(40, 10))\n",
        "ax[0].set_title(\"Reward\")\n",
        "r = ax[0].imshow(reward)\n",
        "plt.colorbar(r, ax=ax[0])\n",
        "\n",
        "ax[1].set_title(\"Initial state distribution\")\n",
        "mu_ = ax[1].imshow(initial.reshape((width, width)))\n",
        "plt.colorbar(mu_, ax=ax[1])\n",
        "\n",
        "ax[2].set_title(\"Expert Soft Value Function\")\n",
        "v_ = ax[2].imshow(expert_v.reshape((width, width)))\n",
        "plt.colorbar(v_, ax=ax[2])\n",
        "\n",
        "ax[3].set_title(\"Expert Stationary Distribution\")\n",
        "rho_ = ax[3].imshow(expert_rho.reshape((width, width)))\n",
        "plt.colorbar(rho_, ax=ax[3])\n",
        "\n",
        "ax_ = ax[4]\n",
        "ax_.set_title(\"Expert Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(expert_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax[5].set_title(\"Dataset Histogram\")\n",
        "rho_ = ax[5].imshow(experience.sum(0).sum(1).reshape((width, width)))\n",
        "plt.colorbar(rho_, ax=ax[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "g06Tg9EEWBGu"
      },
      "outputs": [],
      "source": [
        "# @title Behavoural Cloning (BC)\n",
        "\n",
        "def fit_policy(data, prior):\n",
        "  state_action_counts = data.sum(0)\n",
        "  unnormalised = state_action_counts + jnp.log(prior)\n",
        "  return jnp.exp(unnormalised - logsumexp(unnormalised, axis=1, keepdims=True))\n",
        "\n",
        "bc_pi = fit_policy(experience, prior_policy) + EPS\n",
        "bc_rho = stationary_distribution(initial, transition_matrix, bc_pi, discount)\n",
        "bc_windy_rho = stationary_distribution(initial, windy_transition_matrix, bc_pi, discount)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"BC Stationary Distribution\")\n",
        "fig = ax_.imshow(bc_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"BC Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(bc_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVvbYr5PMsNT"
      },
      "outputs": [],
      "source": [
        "#@title Coherent Soft Imitation Learning (CSIL)\n",
        "log_ratio = jnp.log(bc_pi) - jnp.log(prior_policy)\n",
        "csil_alpha = 1.\n",
        "csil_reward = csil_alpha  * log_ratio\n",
        "\n",
        "csil_alpha = 1e-3\n",
        "csil_pi, csil_q, csil_v = soft_policy_iteration(csil_reward, transition_matrix, initial,  prior_policy, alpha=csil_alpha, gamma=discount, pi_init=bc_pi, q_init=csil_reward)\n",
        "rho_pcirl = stationary_distribution(initial, transition_matrix, csil_pi, discount)\n",
        "windy_rho_pcirl = stationary_distribution(initial, windy_transition_matrix, csil_pi, discount)\n",
        "\n",
        "csil_R_det = policy_eval(csil_pi, rho_pcirl)\n",
        "csil_R_windy = policy_eval(csil_pi, windy_rho_pcirl)\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(40, 10))\n",
        "\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"CSIL Soft Value Function\")\n",
        "fig = ax_.imshow(csil_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"CSIL Stationary Distribution\")\n",
        "fig = ax_.imshow(rho_pcirl.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"CSIL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(windy_rho_pcirl.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Expert Policy\")\n",
        "fig = ax_.imshow(expert_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"CSIL reward\")\n",
        "fig = ax_.imshow(csil_reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"BC Policy\")\n",
        "fig = ax_.imshow(bc_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"CSIL Policy\")\n",
        "fig = ax_.imshow(csil_pi)\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0hkSNdXYgZK"
      },
      "outputs": [],
      "source": [
        "#@title Offline Classifier-style IRL (i.e. ORIL, SQIL, ...)\n",
        "classifier_reward = jnp.array(experience.sum(0) \u003e 0.).astype(np.float32)\n",
        "new_alpha = 0.01\n",
        "\n",
        "classifier_pi, classifier_q, classifier_v = soft_policy_iteration(classifier_reward, transition_matrix, initial, prior_policy, alpha=new_alpha, gamma=discount, pi_init=bc_pi)\n",
        "classifier_rho = stationary_distribution(initial, transition_matrix, classifier_pi, discount)\n",
        "classifier_windy_rho = stationary_distribution(initial, windy_transition_matrix, classifier_pi, discount)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Classifier-based Reward\")\n",
        "fig = ax_.imshow(classifier_reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"Classifier-based IRL Stationary Distribution\")\n",
        "fig = ax_.imshow(classifier_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"Classifier-based IRL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(classifier_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"IRL Value Function\")\n",
        "fig = ax_.imshow(classifier_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aihAnTdX6JO"
      },
      "outputs": [],
      "source": [
        "#@title Maximum Entropy IRL\n",
        "def max_ent_irl(demonstratioon_distribution, transition, initial,  prior_policy, alpha, gamma, n_iters=500, r_lr=1):\n",
        "  demonstratioon_distribution = demonstratioon_distribution / demonstratioon_distribution.sum()\n",
        "  d_state, d_action, _ = transition.shape\n",
        "  meirl_reward = jnp.zeros((d_state, d_action))\n",
        "  errors = []\n",
        "  for _ in range(n_iters):\n",
        "    pi_irl, q, v = soft_policy_iteration(meirl_reward, transition, initial, prior_policy, alpha=alpha, gamma=gamma, n_iters=100)\n",
        "    irl_rho = stationary_distribution(initial, transition, pi_irl, discount, n_iters=100)\n",
        "    irl_distribution = jnp.diag(irl_rho) @ pi_irl\n",
        "    error = (demonstratioon_distribution - irl_distribution)\n",
        "    violation = (error ** 2).mean()\n",
        "    errors += [violation]\n",
        "    if len(errors) \u003e 1 and (abs(errors[-1] - errors[-2]) / abs(errors[-2]) \u003c 1e-5):\n",
        "      break\n",
        "    meirl_reward += r_lr * error\n",
        "  return pi_irl, q, v, meirl_reward\n",
        "\n",
        "alpha_meirl = 1.\n",
        "meirl_pi, meirl_q, meirl_v, meirl_reward = max_ent_irl(expert_distribution, transition_matrix, initial, prior_policy, alpha_meirl, gamma)\n",
        "\n",
        "meirl_rho = stationary_distribution(initial, transition_matrix, meirl_pi, discount)\n",
        "meirl_windy_rho = stationary_distribution(initial, windy_transition_matrix, meirl_pi, discount)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"ME-IRL Policy\")\n",
        "fig = ax_.imshow(meirl_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"ME-IRL Q\")\n",
        "fig = ax_.imshow(meirl_q)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"ME-IRL R\")\n",
        "fig = ax_.imshow(meirl_reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"True R\")\n",
        "fig = ax_.imshow(reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"ME-IRL R\")\n",
        "fig = ax_.imshow(meirl_reward.mean(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Expert Distribution\")\n",
        "fig = ax_.imshow(expert_distribution.sum(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"ME-IRL Stationary Distribution\")\n",
        "fig = ax_.imshow(meirl_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"ME-IRL IRL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(meirl_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"ME-IRL Value Function\")\n",
        "fig = ax_.imshow(meirl_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV8TYQ4_X9MQ"
      },
      "outputs": [],
      "source": [
        "# @title Generative adversarial imitation learning (GAIL)\n",
        "\n",
        "classifier_reward = 1. - jnp.array(experience.sum(0) \u003e 0.).astype(np.float32)\n",
        "\n",
        "@jax.jit\n",
        "def discrimator_loss(weights, expert_distribution, policy_distribution):\n",
        "    classification = jax.nn.sigmoid(weights)\n",
        "    loss = jnp.log(EPS + 1. - classification) * expert_distribution\n",
        "    loss += jnp.log(EPS + classification) * policy_distribution\n",
        "    return loss.sum()\n",
        "\n",
        "discrimator_grad = jax.value_and_grad(discrimator_loss)\n",
        "\n",
        "def gail(expert_distribution, transition, initial,  prior_policy, alpha, gamma, n_iters=100, r_lr=1):\n",
        "  expert_distribution = expert_distribution / expert_distribution.sum()\n",
        "  d_state, d_action, _ = transition.shape\n",
        "  discriminator_weights = classifier_reward\n",
        "  for _ in range(n_iters):\n",
        "    gail_reward = jnp.log(EPS + jax.nn.sigmoid(discriminator_weights))\n",
        "    pi_irl, q, v = soft_policy_iteration(gail_reward, transition_matrix, initial, prior_policy, alpha=alpha, gamma=gamma, n_iters=100)\n",
        "    pi_rho = stationary_distribution(initial, transition_matrix, pi_irl, discount, n_iters=100)\n",
        "    irl_distribution = jnp.diag(pi_rho) @ pi_irl\n",
        "    for _ in range(200):\n",
        "      r_loss, r_grad = discrimator_grad(discriminator_weights, expert_distribution, irl_distribution)\n",
        "      discriminator_weights -= r_lr * r_grad\n",
        "    print(r_loss)\n",
        "  pi, q, v = soft_policy_iteration(gail_reward, transition_matrix, initial, prior_policy, alpha=alpha, gamma=gamma, n_iters=100)\n",
        "  return pi, q, v, jnp.log(EPS + jax.nn.sigmoid(discriminator_weights))\n",
        "\n",
        "gail_alpha = 1e-3\n",
        "gail_pi, gail_q, gail_v, gail_reward = gail(expert_distribution, transition_matrix, initial, prior_policy, gail_alpha, gamma)\n",
        "\n",
        "gail_rho = stationary_distribution(initial, transition_matrix, gail_pi, discount)\n",
        "gail_windy_rho = stationary_distribution(initial, windy_transition_matrix, gail_pi, discount)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"GAIL Policy\")\n",
        "fig = ax_.imshow(gail_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"GAIL Q\")\n",
        "fig = ax_.imshow(gail_q)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"GAIL R\")\n",
        "fig = ax_.imshow(gail_reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"True R\")\n",
        "fig = ax_.imshow(reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[4]\n",
        "ax_.set_title(\"GAIL R\")\n",
        "fig = ax_.imshow(gail_reward.mean(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Expert Distribution\")\n",
        "fig = ax_.imshow(expert_distribution.sum(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"GAIL Stationary Distribution\")\n",
        "fig = ax_.imshow(gail_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"GAIL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(gail_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"GAIL Value Function\")\n",
        "fig = ax_.imshow(gail_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-IWnLpNaOV3"
      },
      "outputs": [],
      "source": [
        "# @title Inverse Soft Q Learning (IQLearn)\n",
        "convex_reg = lambda x: x - 0.25 * x ** 2\n",
        "\n",
        "@jax.jit\n",
        "def iq_learn_loss(q, expert_distribution, pi, transition, initial, prior_policy, discount, alpha):\n",
        "    pi_kl = jnp.sum(pi * (jnp.log(prior_policy) - jnp.log(EPS + pi)), axis=1)\n",
        "    v = jax.lax.stop_gradient(jnp.sum(q * pi, axis=1) + alpha * pi_kl)\n",
        "    imp_reward = q - gamma * jnp.sum(transition * jnp.tile(v, (d_state, d_action, 1)), 2)\n",
        "    objective = (expert_distribution * convex_reg(imp_reward)).sum() - (1-gamma) * jnp.sum(initial * v)\n",
        "    return -objective\n",
        "\n",
        "iq_learn_grad = jax.grad(iq_learn_loss)\n",
        "\n",
        "def inverse_q_learning(expert_distribution, transition, initial,  prior_policy, alpha, gamma, n_iters=1000, q_lr=1e-2):\n",
        "  d_state, d_action, _ = transition.shape\n",
        "  q_ = jnp.zeros((d_state, d_action))\n",
        "  pi_ = jnp.ones(((d_state, d_action))) / d_action\n",
        "  fig, ax = plt.subplots(1, 5, figsize=(40, 10))\n",
        "  d = n_iters // 5\n",
        "  for i in tqdm(range(n_iters)):\n",
        "    pi_kl = jnp.sum(pi_ * (jnp.log(EPS + pi_) - jnp.log(prior_policy)), axis=1)\n",
        "    v_ = jnp.sum(q_ * pi_, axis=1) - alpha * pi_kl\n",
        "    if i % d == 0:\n",
        "      ax_ = ax[i // d]\n",
        "      ax_.set_title(i)\n",
        "      fig = ax_.imshow(v_.reshape((width, width)))\n",
        "      plt.colorbar(fig, ax=ax_)\n",
        "    q_grad = iq_learn_grad(q_, expert_distribution, pi_, transition_matrix, initial, prior_policy, discount, alpha)\n",
        "    q_ = q_ - q_lr * q_grad\n",
        "    pi_ = softmax(q_, alpha, prior_policy)\n",
        "\n",
        "  pi_kl = jnp.sum(pi_ * (jnp.log(EPS + pi_) - jnp.log(prior_policy)), axis=1)\n",
        "  v_ = jnp.sum(q_ * pi_, axis=1) - alpha * pi_kl\n",
        "  imp_reward = q_ - gamma * jnp.sum(transition * jnp.tile(v_, (d_state, d_action, 1)), 2)\n",
        "  return pi_, q_, v_, imp_reward\n",
        "\n",
        "iq_alpha = 1e-3\n",
        "iq_pi, iq_q, iq_v, iq_r = inverse_q_learning(expert_distribution, transition_matrix, initial, prior_policy, iq_alpha, gamma)\n",
        "\n",
        "iq_rho = stationary_distribution(initial, transition_matrix, iq_pi, discount)\n",
        "iq_windy_rho = stationary_distribution(initial, windy_transition_matrix, iq_pi, discount)\n",
        "\n",
        "iq_v = iq_v\n",
        "iq_windy_rho = iq_windy_rho\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"IQ-Learn Policy\")\n",
        "fig = ax_.imshow(iq_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"IQ-Learn Q\")\n",
        "fig = ax_.imshow(iq_q)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"IQ-Learn R\")\n",
        "fig = ax_.imshow(iq_r)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"True Reward R\")\n",
        "fig = ax_.imshow(reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Expert Distribution\")\n",
        "fig = ax_.imshow(expert_distribution.sum(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"IQ-Learn IRL Stationary Distribution\")\n",
        "fig = ax_.imshow(iq_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"IQ-Learn IRL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(iq_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"IQ-Learn Value Function\")\n",
        "fig = ax_.imshow(iq_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgAwmsu0Vbjd"
      },
      "outputs": [],
      "source": [
        "# @title Proximal Point Imitation Learning (PPIL)\n",
        "\n",
        "@jax.jit\n",
        "def dual(q, r, expert_distribution, policy_distribution, pi, transition, initial, prior_policy, discount, alpha):\n",
        "    expert_reward = (r * expert_distribution).sum()\n",
        "    v = -alpha * jax.scipy.special.logsumexp(-q / alpha + jnp.log(EPS + prior_policy), axis=1)\n",
        "    q_target = r + discount * jnp.sum(transition * jnp.tile(v, (d_state, d_action, 1)), 2)\n",
        "    be = q_target - q\n",
        "    w = jax.lax.stop_gradient(jax.nn.softmax(1e-2 * be))\n",
        "    sbe = (policy_distribution * w * be).sum()\n",
        "    dual = expert_reward - sbe - (1 - discount) * jnp.sum(initial * v)\n",
        "    return -dual\n",
        "\n",
        "dual_grad = jax.value_and_grad(dual, argnums=(0, 1))\n",
        "\n",
        "def proximal_point_imitiation_learning(expert_distribution, transition, initial,  prior_policy, alpha, gamma, n_iters=250, lr=1e-2):\n",
        "  d_state, d_action, _ = transition.shape\n",
        "  q_ = jnp.zeros((d_state, d_action))\n",
        "  r_ = jnp.zeros((d_state, d_action))\n",
        "  pi_ = prior_policy\n",
        "  for j in tqdm(range(25)):\n",
        "    policy_distribution = stationary_distribution(initial, transition_matrix, pi_, discount, n_iters=10)\n",
        "    policy_distribution = jnp.diag(policy_distribution) @ pi_\n",
        "    for i in tqdm(range(n_iters)):\n",
        "      d, grads = dual_grad(q_, r_, expert_distribution, policy_distribution, pi_, transition_matrix, initial, pi_, discount, alpha)\n",
        "      q_grad, r_grad = grads\n",
        "      q_ = q_ - lr * q_grad\n",
        "      r_ = r_ - lr * r_grad\n",
        "      r_ = r_ / (r_ ** 2).sum()\n",
        "    pi_ = softmax(q_, alpha, prior_policy)\n",
        "\n",
        "  v_ = -alpha * jax.scipy.special.logsumexp(-q_ / alpha + jnp.log(EPS + pi_), axis=1)\n",
        "  return pi_, q_, v_, r_\n",
        "\n",
        "alpha_ppil = 1e-3\n",
        "ppil_pi, ppil_q, ppil_v, ppil_r = proximal_point_imitiation_learning(expert_distribution, transition_matrix, initial, prior_policy, alpha_ppil, gamma)\n",
        "\n",
        "ppil_rho = stationary_distribution(initial, transition_matrix, ppil_pi, discount)\n",
        "ppil_windy_rho = stationary_distribution(initial, windy_transition_matrix, ppil_pi, discount)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"PPIL Policy\")\n",
        "fig = ax_.imshow(ppil_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"PPIL Q\")\n",
        "fig = ax_.imshow(ppil_q)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"PPIL R\")\n",
        "fig = ax_.imshow(ppil_r)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"True Reward R\")\n",
        "fig = ax_.imshow(reward)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Expert Distribution\")\n",
        "fig = ax_.imshow(expert_distribution.sum(1).reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"PPIL IRL Stationary Distribution\")\n",
        "fig = ax_.imshow(ppil_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"PPIL IRL Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(ppil_windy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"PPIL Value Function\")\n",
        "fig = ax_.imshow(ppil_v.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqRJieG1y4dU"
      },
      "outputs": [],
      "source": [
        "# @title Report results\n",
        "\n",
        "import tabulate\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "greedy_pi = jnp.array(expert_pi == jnp.max(expert_pi, axis=1, keepdims=True)).astype(np.float32)\n",
        "greedy_pi = greedy_pi / greedy_pi.sum(axis=1, keepdims=True)\n",
        "greedy_rho = stationary_distribution(initial, transition_matrix, greedy_pi, discount)\n",
        "windy_greedy_rho = stationary_distribution(initial, windy_transition_matrix, greedy_pi, discount)\n",
        "\n",
        "\n",
        "pi_R_det = policy_eval(expert_pi, expert_rho)\n",
        "pi_R_windy = policy_eval(expert_pi, expert_windy_rho)\n",
        "\n",
        "greedy_R_det = policy_eval(greedy_pi, greedy_rho)\n",
        "greedy_R_windy = policy_eval(greedy_pi, windy_greedy_rho)\n",
        "\n",
        "bc_pi_R_det = policy_eval(bc_pi, bc_rho)\n",
        "bc_pi_R_windy = policy_eval(bc_pi, bc_windy_rho)\n",
        "\n",
        "meirl_pi_R_det = policy_eval(meirl_pi, meirl_rho)\n",
        "meirl_pi_R_windy = policy_eval(meirl_pi, meirl_windy_rho)\n",
        "\n",
        "gail_pi_R_det = policy_eval(gail_pi, gail_rho)\n",
        "gail_pi_R_windy = policy_eval(gail_pi, gail_windy_rho)\n",
        "\n",
        "csil_R_det = policy_eval(csil_pi, rho_pcirl)\n",
        "csil_R_windy = policy_eval(csil_pi, windy_rho_pcirl)\n",
        "\n",
        "irl_R_det = policy_eval(classifier_pi, classifier_rho)\n",
        "irl_R_windy = policy_eval(classifier_pi, classifier_windy_rho)\n",
        "\n",
        "iq_R_det = policy_eval(iq_pi, iq_rho)\n",
        "iq_R_windy = policy_eval(iq_pi, iq_windy_rho)\n",
        "\n",
        "ppil_R_det = policy_eval(ppil_pi, ppil_rho)\n",
        "ppil_R_windy = policy_eval(ppil_pi, ppil_windy_rho)\n",
        "\n",
        "data =  [[\"Greedy Soft Expert \", greedy_R_det, greedy_R_windy],\n",
        "         [\"Soft Expert \", pi_R_det, pi_R_windy],\n",
        "         [\"BC \", bc_pi_R_det, bc_pi_R_windy],\n",
        "         [\"CLASSIFIER \", irl_R_det, irl_R_windy],\n",
        "         [\"ME-IRL \", meirl_pi_R_det, meirl_pi_R_windy],\n",
        "         [\"GAIL \", gail_pi_R_det, gail_pi_R_windy],\n",
        "         [\"IQLEARN \", iq_R_det, iq_R_windy],\n",
        "         [\"PPIL \", ppil_R_det, ppil_R_windy],\n",
        "         [\"CSIL \", csil_R_det, csil_R_windy]]\n",
        "\n",
        "table = tabulate.tabulate(data, tablefmt='html', floatfmt=\".3f\", headers=[\"\",\"Deterministic\",\"Windy\"],)\n",
        "display(HTML(table))\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
        "ax_ = ax[0]\n",
        "ax_.set_title(\"Greedy Stationary Distribution\")\n",
        "fig = ax_.imshow(greedy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "ax_ = ax[1]\n",
        "ax_.set_title(\"Greedy Stationary Distribution (Windy)\")\n",
        "fig = ax_.imshow(windy_greedy_rho.reshape((width, width)))\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "ax_ = ax[2]\n",
        "ax_.set_title(\"Greedy Policy\")\n",
        "fig = ax_.imshow(greedy_pi)\n",
        "plt.colorbar(fig, ax=ax_)\n",
        "ax_ = ax[3]\n",
        "ax_.set_title(\"Soft Policy\")\n",
        "fig = ax_.imshow(expert_pi)\n",
        "plt.colorbar(fig, ax=ax_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvuw8h1ZohH4"
      },
      "outputs": [],
      "source": [
        "#@title Plotting imports and constants\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base = 10\n",
        "small_offset = 2\n",
        "constrained_layout = True\n",
        "tight_layout = True\n",
        "_GOLDEN_RATIO = (5.0**0.5 - 1.0) / 2.0\n",
        "width_in = 6.9\n",
        "height_in = width_in\n",
        "height_per_row = height_in // 4\n",
        "pad_inches = 0.015\n",
        "from matplotlib import rc\n",
        "rc('mathtext', fontset='cm')\n",
        "\n",
        "mpl_params = {\n",
        "    'figure.constrained_layout.use': True,\n",
        "    \"lines.markersize\": 0.1,\n",
        "    \"lines.linewidth\": 1.0,\n",
        "    \"font.family\": \"Crimson Text\",\n",
        "    \"text.usetex\": False,\n",
        "    \"font.serif\": [\"Crimson Text\"] + plt.rcParams['font.serif'],\n",
        "    \"mathtext.fontset\": \"stix\",  # free ptmx replacement, for ICML and NeurIPS\n",
        "    \"mathtext.rm\": \"Times New Roman\",\n",
        "    \"mathtext.it\": \"Times New Roman:italic\",\n",
        "    \"mathtext.bf\": \"Times New Roman:bold\",\n",
        "    \"font.size\": base,\n",
        "    \"axes.labelsize\": base,\n",
        "    \"legend.fontsize\": base - small_offset,\n",
        "    \"xtick.labelsize\": base - small_offset,\n",
        "    \"ytick.labelsize\": base - small_offset,\n",
        "    \"axes.titlesize\": base,\n",
        "    \"figure.constrained_layout.use\": constrained_layout,\n",
        "    \"figure.autolayout\": tight_layout,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "    \"savefig.pad_inches\": pad_inches,\n",
        "}\n",
        "\n",
        "plt.rcParams.update(mpl_params)\n",
        "\n",
        "title_params = {'fontfamily':'monospace'}\n",
        "\n",
        "# Colorblind-safe and print-friendly colors selected from colorbrewer2.org\n",
        "YELLOW = '#F2E34C'\n",
        "GREEN = '#82b392'\n",
        "AQUA = '#41b6c4'\n",
        "BLUE = '#2c7fb8'\n",
        "ROYALBLUE = '#253494'\n",
        "PURPLE = '#998ec3'\n",
        "\n",
        "RED = '#d7191c'\n",
        "LIGHTRED = '#ffcccb'\n",
        "ORANGE = '#fdae61'\n",
        "LIGHTBLUE = '#abd9e9'\n",
        "MEDBLUE = '#2c7bb6'\n",
        "\n",
        "LIGHTORANGE = '#fed98e'\n",
        "MEDORANGE = '#fe9929'\n",
        "DARKORANGE = '#cc4c02'\n",
        "\n",
        "GREY = '#808080'\n",
        "\n",
        "OPACITY = 0.2  # for uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1HBqaJeohyw"
      },
      "outputs": [],
      "source": [
        "#@title Plot and save results\n",
        "from colabtools import fileedit\n",
        "RESULTS = {\n",
        "    \"expert\": (expert_v, greedy_rho, windy_greedy_rho),\n",
        "    \"bc\": (None, bc_rho, bc_windy_rho),\n",
        "    \"csil\": (csil_v, rho_pcirl, windy_rho_pcirl),\n",
        "    \"classifier\": (classifier_v, classifier_rho, classifier_windy_rho),\n",
        "    \"me-irl\": (meirl_v, meirl_rho, meirl_windy_rho),\n",
        "    \"gail\": (gail_v, gail_rho, gail_windy_rho),\n",
        "    \"iqlearn\": (iq_v, iq_rho, iq_windy_rho),\n",
        "    \"ppil\": (ppil_v, ppil_rho, ppil_windy_rho),\n",
        "}\n",
        "\n",
        "exp = f\"{'sparse' if SPARSE_INITIAL_STATES else 'dense'}_{'easy' if EASY_REWARD else 'hard'}\"\n",
        "\n",
        "for name, objs in RESULTS.items():\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(width_in, width_in // 3), sharex=True, sharey=True, gridspec_kw={'wspace': 0.1})\n",
        "  for ax_ in ax:\n",
        "    ax_.set_xticklabels([])\n",
        "    ax_.set_yticklabels([])\n",
        "    ax_.set_xticks([])\n",
        "    ax_.set_yticks([])\n",
        "  v, rho, wrho = objs\n",
        "\n",
        "  ax_ = ax[0]\n",
        "  if v is not None:\n",
        "    ax_.set_title(\"Value function\")\n",
        "    ax_.imshow(v.reshape((width, width)))\n",
        "  else:\n",
        "    ax_.axis('off')\n",
        "\n",
        "  ax_ = ax[1]\n",
        "  ax_.set_title(\"Nominal stationary distribution\")\n",
        "  ax_.imshow(rho.reshape((width, width)))\n",
        "  ax_ = ax[2]\n",
        "  ax_.set_title(\"Windy stationary distribution\")\n",
        "  ax_.imshow(wrho.reshape((width, width)))\n",
        "\n",
        "\n",
        "  filename = f'{exp}_{name}.pdf'\n",
        "  fig.savefig(filename, bbox_inches = 'tight')\n",
        "  fileedit.download_file(filename, ephemeral=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "kind": "private"
      },
      "name": "soft_policy_iteration.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
